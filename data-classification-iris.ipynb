{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import neccessary labraries","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import datasets\nfrom sklearn.neighbors import KNeighborsClassifier\n\n","metadata":{"execution":{"iopub.status.busy":"2023-02-23T17:20:29.968924Z","iopub.execute_input":"2023-02-23T17:20:29.969334Z","iopub.status.idle":"2023-02-23T17:20:30.788432Z","shell.execute_reply.started":"2023-02-23T17:20:29.969294Z","shell.execute_reply":"2023-02-23T17:20:30.787232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# load dataset","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/iris-data/IRIS_data.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-02-23T17:21:11.373160Z","iopub.execute_input":"2023-02-23T17:21:11.373557Z","iopub.status.idle":"2023-02-23T17:21:11.394006Z","shell.execute_reply.started":"2023-02-23T17:21:11.373527Z","shell.execute_reply":"2023-02-23T17:21:11.393009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we have four features:  \tsepal_length \tsepal_width \tpetal_length \tpetal_width and target value (class) : species","metadata":{}},{"cell_type":"markdown","source":"# Analyze and visualize the dataset","metadata":{}},{"cell_type":"code","source":"# Some basic statistical analysis about the data\ndf.describe()","metadata":{"execution":{"iopub.status.busy":"2023-02-23T17:21:27.024840Z","iopub.execute_input":"2023-02-23T17:21:27.025735Z","iopub.status.idle":"2023-02-23T17:21:27.060637Z","shell.execute_reply.started":"2023-02-23T17:21:27.025695Z","shell.execute_reply":"2023-02-23T17:21:27.059432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"setosa = df[df.species == \"setosa\"]\nversicolor = df[df.species=='versicolor']\nvirginica = df[df.species=='virginica']","metadata":{"execution":{"iopub.status.busy":"2023-02-23T17:21:55.753897Z","iopub.execute_input":"2023-02-23T17:21:55.754885Z","iopub.status.idle":"2023-02-23T17:21:55.761662Z","shell.execute_reply.started":"2023-02-23T17:21:55.754841Z","shell.execute_reply":"2023-02-23T17:21:55.760753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The df[df.species == \"setosa\"] code filters the DataFrame df to only include rows where the species column equals \"setosa\". This creates a new DataFrame called setosa, which contains all rows where the species is \"setosa\". Similarly, df[df.species=='versicolor'] creates a new DataFrame called versicolor, and df[df.species=='virginica'] creates a new DataFrame called virginica.","metadata":{}},{"cell_type":"code","source":"# Visualize the whole dataset\nsns.pairplot(df, hue='species')","metadata":{"execution":{"iopub.status.busy":"2023-02-23T17:22:12.405845Z","iopub.execute_input":"2023-02-23T17:22:12.406266Z","iopub.status.idle":"2023-02-23T17:22:17.325022Z","shell.execute_reply.started":"2023-02-23T17:22:12.406229Z","shell.execute_reply":"2023-02-23T17:22:17.324006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this case, we set the hue to the 'species' column in the df DataFrame, so that each species is plotted with a different color.creates a pair plot of the data, with different colors indicating the different species of the flowers.In this case, we set the hue to the 'species' column in the df DataFrame, so that each species is plotted with a different color.\n\nA pair plot is a grid of scatter plots that shows the relationship between pairs of variables in a dataset. Each row and column of the grid represents a different variable, and the diagonal plots show a histogram or density estimate of each variable.","metadata":{}},{"cell_type":"markdown","source":"From the figure, we can find that iris-setosa is well separated from the other two flowers.And iris virginica is the longest flower and iris setosa is the shortest.During your exercises and courseworks, you are recommended to do data analysis and visualization to have a better understanding of your data which will help you choosen suitable machine learning algorithm.#","metadata":{}},{"cell_type":"code","source":"# Separate features and target  \ndata = df.values\nX = data[:,0:4]\nY = data[:,4]","metadata":{"execution":{"iopub.status.busy":"2023-02-23T17:23:23.625241Z","iopub.execute_input":"2023-02-23T17:23:23.625660Z","iopub.status.idle":"2023-02-23T17:23:23.631748Z","shell.execute_reply.started":"2023-02-23T17:23:23.625623Z","shell.execute_reply":"2023-02-23T17:23:23.630294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This code snippet is a common example in machine learning, particularly in the field of data preparation and feature selection using Python and the NumPy library. Assuming df is a Pandas DataFrame object that contains data for different iris flower species, the df.values code converts the DataFrame to a NumPy array, which is a common data structure used in machine learning algorithms.\n\nThe next two lines of code, X = data[:,0:4] and Y = data[:,4], separate the input features (X) and output labels (Y) from the NumPy array. In this case, we assume that the first four columns of the array (data[:,0:4]) contain the input features, and the fifth column (data[:,4]) contains the output labels.\n\nThe : operator in NumPy is used to select all rows of a particular dimension in an array, and the 0:4 range specifies that we want to select columns 0 through 3 (inclusive) of the array. This creates a new array X that contains only the input features.\n\nSimilarly, data[:,4] selects all rows of the fifth column of the array, which contains the output labels for each data point. This creates a new array Y that contains only the output labels.\n\nBy separating the input features and output labels into separate arrays, we can more easily manipulate and preprocess the data for use in machine learning algorithms. For example, we can use techniques such as feature scaling, feature selection, or dimensionality reduction on the input features, or we can split the data into training and testing sets for use in a machine learning model.\n\n\n\n","metadata":{}},{"cell_type":"markdown","source":"# Model training\nhere we only use KNN as an example, you are encouraged to try other machine learning algorithms during your exercises and compare the performance.","metadata":{}},{"cell_type":"code","source":"# Split the data to train and test dataset.\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, Y)","metadata":{"execution":{"iopub.status.busy":"2023-02-23T17:23:37.351124Z","iopub.execute_input":"2023-02-23T17:23:37.351563Z","iopub.status.idle":"2023-02-23T17:23:37.359004Z","shell.execute_reply.started":"2023-02-23T17:23:37.351528Z","shell.execute_reply":"2023-02-23T17:23:37.357911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This code snippet is a common example in machine learning, particularly in the field of supervised learning using Python and the scikit-learn library. Assuming X is a NumPy array of input features and Y is a NumPy array of output labels, the train_test_split function from the sklearn.model_selection module is used to split the data into training and testing sets.\n\nThe train_test_split function randomly splits the data into two sets: a training set and a testing set. By default, the function splits the data into 75% training data and 25% testing data, but this can be adjusted using the test_size parameter.\n\nThe function returns four NumPy arrays: X_train, X_test, y_train, and y_test. X_train and y_train contain the training data, while X_test and y_test contain the testing data. The X_train and X_test arrays contain the input features, while the y_train and y_test arrays contain the output labels.\n\nBy splitting the data into training and testing sets, we can use the training set to train a machine learning model and the testing set to evaluate its performance. This helps to prevent overfitting, where the model performs well on the training data but poorly on new data, by testing the model on data that it has not seen during training.","metadata":{}},{"cell_type":"code","source":"# model training\nnn = KNeighborsClassifier(n_neighbors=3)\n#nn = KNeighborsClassifier(n_neighbors=3)\n#nn = KNeighborsClassifier(n_neighbors=5)\n#nn = KNeighborsClassifier(n_neighbors=10)\n#from NN to 3-NN to 5-NN to 10-NN\n#change the number of K \nnn.fit(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2023-02-23T17:23:45.286677Z","iopub.execute_input":"2023-02-23T17:23:45.287140Z","iopub.status.idle":"2023-02-23T17:23:45.297835Z","shell.execute_reply.started":"2023-02-23T17:23:45.287096Z","shell.execute_reply":"2023-02-23T17:23:45.296671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This code snippet is a common example in machine learning, particularly in the field of classification using Python and the scikit-learn library. Assuming X_train is a NumPy array of input features for the training data, y_train is a NumPy array of output labels for the training data, and KNeighborsClassifier is imported from sklearn.neighbors, this code creates an instance of a K-Nearest Neighbors (KNN) classifier with n_neighbors set to 3, and trains it on the training data using the fit method.\n\nThe KNN algorithm is a type of instance-based or lazy learning algorithm, where predictions are made by finding the K closest training examples in the feature space and using the majority class label among the K neighbors to classify the new instance. The value of K is a hyperparameter that controls the number of neighbors used to make predictions.\n\nIn this example, we create four different KNN classifiers with different values of K (3, 5, and 10), and we can choose the best value of K by comparing the performance of the classifiers on the testing data.\n\nAfter creating the KNN classifier with n_neighbors set to 3, we train it on the training data using the fit method. This trains the model to learn the relationships between the input features and the output labels in the training data.\n\nOnce the model is trained, we can use it to make predictions on new data using the predict method. In this case, we have not yet used the model to make predictions on any new data, so this code only trains the model on the training data.\n\n\n\n","metadata":{}},{"cell_type":"markdown","source":"# Model evaluation","metadata":{}},{"cell_type":"code","source":"# Predict from the test dataset\ny_predict = nn.predict(X_test)\n# Calculate the accuracy\nfrom sklearn.metrics import accuracy_score\n# accuracy\naccuracy_score(y_test, y_predict)","metadata":{"execution":{"iopub.status.busy":"2023-02-23T17:24:07.079848Z","iopub.execute_input":"2023-02-23T17:24:07.080701Z","iopub.status.idle":"2023-02-23T17:24:07.091928Z","shell.execute_reply.started":"2023-02-23T17:24:07.080658Z","shell.execute_reply":"2023-02-23T17:24:07.090971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This code snippet is a common example in machine learning, particularly in the field of classification using Python and the scikit-learn library. Assuming nn is a trained KNN classifier, X_test is a NumPy array of input features for the testing data, y_test is a NumPy array of output labels for the testing data, and accuracy_score is imported from sklearn.metrics, this code uses the trained model to make predictions on the testing data using the predict method and calculates the accuracy of the model's predictions using the accuracy_score function.\n\nThe predict method takes an array of input features as input and returns an array of predicted output labels. In this case, we pass the X_test array of input features to the predict method to generate an array of predicted output labels for the testing data.\n\nOnce we have the predicted output labels, we can calculate the accuracy of the model's predictions using the accuracy_score function. The accuracy_score function takes two arrays of output labels as input and returns the accuracy of the predicted labels compared to the true labels.\n\nIn this case, we pass the y_test array of true output labels and the y_predict array of predicted output labels to the accuracy_score function to calculate the accuracy of the model's predictions on the testing data. The resulting value is a float between 0 and 1, where a higher value indicates better performance.\n\n","metadata":{}},{"cell_type":"code","source":"# Importing the classification report\nfrom sklearn.metrics import classification_report","metadata":{"execution":{"iopub.status.busy":"2023-02-23T17:24:13.913442Z","iopub.execute_input":"2023-02-23T17:24:13.913970Z","iopub.status.idle":"2023-02-23T17:24:13.919546Z","shell.execute_reply.started":"2023-02-23T17:24:13.913938Z","shell.execute_reply":"2023-02-23T17:24:13.918141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The classification_report function is a useful tool in machine learning for evaluating the performance of a classification model. It is part of the sklearn.metrics module in Python's scikit-learn library.\n\nAssuming the model has already been trained and tested, we can use the classification_report function to generate a report that summarizes the precision, recall, F1-score, and support for each class in the predicted output labels. These metrics provide a more detailed evaluation of the model's performance than just the overall accuracy.","metadata":{}},{"cell_type":"markdown","source":"Then, assuming y_test is a NumPy array of true output labels for the testing data and y_predict is a NumPy array of predicted output labels generated by the model, we can call the classification_report function as follows:","metadata":{}},{"cell_type":"code","source":"print(classification_report(y_test, y_predict))","metadata":{"execution":{"iopub.status.busy":"2023-02-23T17:24:21.641035Z","iopub.execute_input":"2023-02-23T17:24:21.641469Z","iopub.status.idle":"2023-02-23T17:24:21.656914Z","shell.execute_reply.started":"2023-02-23T17:24:21.641432Z","shell.execute_reply":"2023-02-23T17:24:21.655422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This will print a report that summarizes the precision, recall, F1-score, and support for each class in the predicted output labels. The support is the number of occurrences of each class in the true output labels. The precision is the proportion of true positives among all predicted positives, and the recall is the proportion of true positives among all actual positives. The F1-score is the harmonic mean of precision and recall, and is a measure of overall accuracy that takes into account both precision and recall.","metadata":{}},{"cell_type":"code","source":"# confusion matrix\nprint(confusion_matrix(y_test,y_predict))","metadata":{"execution":{"iopub.status.busy":"2023-02-23T17:24:30.421231Z","iopub.execute_input":"2023-02-23T17:24:30.421644Z","iopub.status.idle":"2023-02-23T17:24:30.430329Z","shell.execute_reply.started":"2023-02-23T17:24:30.421613Z","shell.execute_reply":"2023-02-23T17:24:30.429224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The confusion_matrix function is another useful tool in machine learning for evaluating the performance of a classification model. It is part of the sklearn.metrics module in Python's scikit-learn library.\n\nAssuming the model has already been trained and tested, we can use the confusion_matrix function to generate a confusion matrix that summarizes the number of true positives, false positives, true negatives, and false negatives for each class in the predicted output labels. A confusion matrix can help us identify which classes the model is performing well on and which classes it is not.\n\n assuming y_test is a NumPy array of true output labels for the testing data and y_predict is a NumPy array of predicted output labels generated by the model\n \n This will print a confusion matrix that summarizes the number of true positives, false positives, true negatives, and false negatives for each class in the predicted output labels. The rows of the matrix represent the true classes, and the columns represent the predicted classes. The diagonal elements of the matrix represent the number of true positives, while the off-diagonal elements represent the number of false positives, false negatives, and true negatives.","metadata":{}},{"cell_type":"markdown","source":"This function might be used in your coursework task for the results visualization.","metadata":{}},{"cell_type":"code","source":"def confusionM(y_true,y_predict,target_names):\n#function for confusion matrix visualisation\n    cMatrix = confusion_matrix(y_true,y_predict)\n    df_cm = pd.DataFrame(cMatrix,index=target_names,columns=target_names)\n    plt.figure(figsize = (6,4))\n    cm = sns.heatmap(df_cm,annot=True,fmt=\"d\")\n    cm.yaxis.set_ticklabels(cm.yaxis.get_ticklabels(),rotation=90)\n    cm.xaxis.set_ticklabels(cm.xaxis.get_ticklabels(),rotation=0)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n# get the 3 class names\nclass_names = df.species.unique()\n\nconfusionM(y_test,y_predict,class_names)","metadata":{"execution":{"iopub.status.busy":"2023-02-17T09:12:43.463607Z","iopub.execute_input":"2023-02-17T09:12:43.464099Z","iopub.status.idle":"2023-02-17T09:12:43.795012Z","shell.execute_reply.started":"2023-02-17T09:12:43.464058Z","shell.execute_reply":"2023-02-17T09:12:43.793687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The confusionM function is a custom function for visualizing the confusion matrix for a classification model. It takes three arguments:\n\ny_true: a NumPy array of true output labels for the testing data\ny_predict: a NumPy array of predicted output labels generated by the model\ntarget_names: a list of strings representing the names of the classes\nThe function first generates a confusion matrix using the confusion_matrix function from scikit-learn. It then creates a Pandas DataFrame to store the confusion matrix, using the class names as the row and column labels. Finally, it creates a heatmap using the Seaborn library to visualize the confusion matrix.\n\nTo use this function, we need to define it first, as shown in the code snippet you provided. \n\nThis will generate a visualization of the confusion matrix for the predicted output labels, using the true output labels as the reference. The rows of the matrix represent the true classes, and the columns represent the predicted classes. The color of each square in the matrix represents the number of instances that were classified as the corresponding class. The diagonal elements of the matrix represent the number of true positives, while the off-diagonal elements represent the number of false positives, false negatives, and true negatives.\n","metadata":{}}]}